:chapterNumber: 6
:chapterId: chapter-06
:sourceDir: ./examples
:sourceSample: TODO.js
:nodeCurrentVersion: v8
:nodeNextVersion: v9
:npmCurrentVersion: v5
:sectnums:
:revdate: {docdate}
:imagesdir: {indir}
:cross: &#x2718;
:tick: &#x2714;
ifdef::env[]
:imagesdir: .
endif::[]

= Déployer notre code

====
.Sommaire
- Déployer une application Node
- Choisir son hébergement
- Améliorer la portabilité
- Démarrer automatiquement nos applications
- Points de vigilance après une mise en ligne
====

[abstract]
--
--

include::../resources/tip-versions.adoc[]
include::../resources/tip-examples.adoc[]


[[deploy]]
== Déployer une application Node

////
Préciser que pour aller rapidement faut aller sur le PaaS (git push)
////

[[deploy.github]]
=== En important du code depuis GitHub

glitch
heroku

TBD.

[[deploy.cli]]
=== Avec un assistant de déploiement

----
$ heroku login
Enter your Heroku credentials:
Email: hi@oncletom.io
Password: *****************
Logged in as hi@oncletom.io
----

[[deploy.git]]
=== En faisant `git push` depuis sa machine

TBD.

[[deploy.clone]]
=== En faisant `git pull` lors d'une session SSH

TBD.


[[deploy.provisioning]]
=== Avec un outil de provisionnement

TBD.

////
- Ansible
////


[[deploy.ci]]
=== En paramétrant un logiciel d'intégration continue

TBD.

////
- Travis Deploy
////


[[hosting]]
== Choisir son hébergement

Où faire vivre nos applications Node en dehors de notre ordinateur servant au développement ?

Nous allons nous intéresser aux différents mécanismes exposant des applications Node au monde extérieur, que ce soit sur une machine dédiée ou sur des services en ligne.

[[paas]]
=== Plate-forme en tant que service (_Platform as a Service_)

Les plates-formes de services *automatisent la configuration* et *facilitent le déploiement* de nos applications Node mais également Ruby, Python et PHP, entres autres.
Elles se spécialisent dans des déploiements rapides, une allocation des ressources flexible, à la demande et en un clic.

Leur philosophie est de *tout penser en terme de ressources et de services*.
On paie pour la capacité informatique nécessaire à un _instant T_ tout en ayant le loisir de réduire ou d'augmenter cette capacité à tout moment sans remettre en cause l'architecture de nos applications.

Chaque instance peut être dimensionnée _à tout moment_ en _puissance_ (CPU, RAM) et en _nombre_ (de 1 à plusieurs instances en parallèle).
La distribution du trafic web est gérée pour nous.

Le principe de déploiement sur les plates-formes de services est réduit au quasi-minimum :

. création d'une nouvelle application sur la plate-forme en ligne ;
. nous _poussons_ notre code applicatif — généralement avec `git` ;
. la plate-forme prépare la mise en ligne en installant les dépendances _npm_ et en démarrant l'application web ;
. notre application est fonctionnelle quelques secondes plus tard.

[format="csv", options="header"]
.Sélection de fournisseurs _PaaS_
|===
Service, Déploiement, Add-ons, Gratuité, Tarif
[URL]#https://zeit.co#, "cli", {cross}, 3 apps, 15$/mois/10 apps
[URL]#https://clever-cloud.com#, "git", {tick}, crédit 20€, 5€/mois/app
[URL]#https://gandi.net/hosting/simple#, "cli/git/SSH", {tick}, 10 jours, 5€/mois/app
[URL]#https://scalingo.com#, "git/GitHub", {tick}, 30 jours, 7€/mois/app
[URL]#https://heroku.com#, "cli/git/GitHub/Dropbox", {tick}, 1000 heures par mois, 7$/mois/app
|===

La startup californienne Zeit ([URL]#https://zeit.co#) édite le service de _déploiements immutables_ nommé _now_ ([URL]#https://zeit.co/now#).
Ce service est focalisée sur l'hébergement de sites statiques, d'applications web Node et de conteneurs Docker.

Sa particularité est de créer une *nouvelle instance d'application par déploiement*.
Chaque déploiement est donc unique.
À nous de désigner le déploiement associé à l'instance de production par l'intermédiaire de la commande `now alias`.

Une fois notre compte créé et notre terminal positionné dans un répertoire de notre ordinateur contenant la racine du projet à déployer, il suffit d'installer le _client now_ et d'initier le déploiement :

[source,bash]
----
$ npm i -g now
$ now
----

[TIP]
.[RemarquePreTitre]#Documentation# now
====
Le fonctionnement de l'application _now_ est illustré et documenté à la fois en ligne et via un terminal de commandes.

- `now --help`
- [URL]#https://zeit.co/docs#
====

Des *ressources complémentaires* sont disponibles auprès d'autres fournisseurs de services.
Leur gestion suit les mêmes principes d'*élasticité à la demande*.
Cela concerne les bases de données _MySQL_, _MariaDB_, _redis_ ou _postgreSQL_ par exemple mais aussi l'envoi d'emails, des moteurs d'indexation de contenus, le monitoring applicatif, l'aggrégation des logs etc.

Des plates-formes comme _Heroku_ ([URL]#https://heroku.com#) et _Clever Cloud_ ([URL]#https://clever-cloud.com#) fédèrent ces ressources sur une place de marché.
L'intégration des ressources en est ainsi facilitée, en quelques clics ou depuis un terminal.
Les _variables d'environnement_ contenant les informations de connexion nous sont communiquées dès que les ressources complémentaires sont provisionnées.
Il ne nous reste alors plus qu'à <<app-configuration,configurer nos applications>> en fonction.

.Ensemble de ressources complémentaires à une application Node hébergée sur Heroku.
image::images/heroku-addons.png[width="85%"]

ifeval::["{backend}" == "html5"]
.Exemple de déploiement applicatif vers Heroku.
video::videos/heroku-deploy.mp4[width="70%"]
endif::[]

ifeval::["{backend}" != "html5"]
.Exemple de déploiement applicatif vers Heroku.
image::images/heroku-deploy.png[width="85%"]
endif::[]

Les plates-formes en tant que service facturent à l'heure consommée.
Elles sont en génral faciles à prendre en main et ne nécessitent pas de connaissance en administration de machine informatique. +
Les instances d'application ont l'avantage d'être rapidement déployées, peuvent être mises en _pause_ et s'adapter au trafic ainsi qu'à notre budget.

Le paiement à l'heure _et_ à l'instance peut rapidement saler l'addition et questionner l'utilisation de ce type de services en lieu et place d'un <<vm-hosting,serveur dédié ou d'une machine virtuelle>>. +
La question est de savoir combien de notre temps nous souhaitons passer à maintenir une infrastructure — du temps qui coûte également de l'argent.


[[shared-hosting]]
=== Hébergement mutualisé

Les hébergements mutualisés sont bon marché car ils se "contentent" de mutualiser un frontal HTTP et de servir des fichiers statiques et des scripts interprétés.
Python ou PHP, par exemple.

(parler d'Alwaysdata)


[[cloud]]
=== Serveur dédié, cloud, VPS, etc.


- [URL]#https://gandi.net/hosting/iaas#
- [URL]#https://ovh.com/fr/#
- [URL]#https://digitalocean.com/#
- [URL]#https://aws.amazon.com/fr/ec2/#

[TIP]
.[RemarquePreTitre]#Alternative# AWS Elastic Beanstalk
====

- [URL]#https://aws.amazon.com/fr/elasticbeanstalk/#
====

[[deploy.docker]]
=== En publiant une image Docker

TBD.

[[lambda]]
=== Invocation éphémère (_Function as a Service_)

TBD.

////
- Lambda
////



== Améliorer la portabilité

Cette section va nous aider à préparer notre application Node pour l'exécuter
sur *d'autres environnements* que notre machine de développement sans changer
une seule ligne de code.

[[configuration]]
=== Configurer sans toucher à notre code

L'*environnement change entre notre machine de développement et la production*, le serveur de tests ou encore une plate-forme de services.
Le nom et paramètres d'accès à une base de données diffèrent, le système d'exploitation n'est pas le même ni même les codes d'accès pour accéder à certaines APIs.
Dans certains cas, nous n'avons même pas la main sur la décision d'un élément de configuration : il nous est fourni par l'environnement.
À nous donc de nous adapter.

Le premier élément que nous souhaitons maitriser est la *bonne version de Node* pour exécuter une application donnée.
Je recommande l'*utilisation d'un environnement par application* ou à défaut, d'une technologie de _conteneurs_ (comme LXC, Docker etc.) pour héberger plusieurs applications sur un même environnement.

Gérer plusieurs applications sur un même environnement implique soit de *_forcer_ toutes les applications* à se baser sur la même version de Node (mauvaise idée) soit à permettre à *chaque application d'expliciter sa préférence* (meilleure idée). +
Les _outils de gestion de version pour Node_ exposent justement un mécanisme pour indiquer la version désirée de *manière déclarative* via un fichier texte – ici, `.nvmrc` dans le cas de <<../chapter-02/index.adoc#nvm,nvm>> (cf. Chapitre 2).

[subs="attributes"]
.{empty}.nvmrc
----
include::.nvmrc[]
----

Les commandes exposées par _nvm_ ont _connaissance_ de la version de Node déclarée dans le fichier `.nvmrc`.
Elles s'adapteront à cette version sauf à demander une version explicite :

[source,bash,subs="attributes"]
----
$ nvm install
$ nvm install {nodeNextVersion}
$ nvm run config/version.js
$ nvm run {nodeNextVersion} config/version.js
$ nvm exec npm start
----

Les cinq précédentes commandes permettent de :

. installer Node {nodeCurrentVersion} – version obtenue via `.npmrc` ;
. installer Node {nodeNextVersion} – version explicite ;
. exécuter le script `config/version.js` avec Node {nodeCurrentVersion} – version obtenue via `.npmrc` ;
. exécuter le script `config/version.js` avec Node {nodeNextVersion} – version explicite ;
. exécuter la commande `npm start` dans un environnement Node {nodeCurrentVersion}.

[CAUTION]
.[RemarquePreTitre]#Performance# Temps de démarrage
====
L'utilisation de _nvm_ entraine une pénalité d'environ _une seconde_ lors du _démarrage_ de l'application.
====

Nous n'avons certainement *pas envie de modifier le code* de notre application pour refléter ces différences.
C'est également l'*horreur de maintenir un fichier de configuration* par environnement.
Alors comment faire ?

L'utilisation de *variables d'environnement pour configurer une application* est la solution la plus aisée à implémenter.
Ces variables sont accessibles via l'objet `process.env` et *ne sont pas changer pendant la durée d'exécution d'une application*.
Voici un échantillon de leurs usages :

* *environnement d'exécution* (voir l'encadré sur `NODE_ENV` ci-après) ;
* *adresses de connexion aux bases de données* (_Data Source Name_, _DSN_) ;
* *URL des ressources et APIs* dont l'application dépend ;
* *autres variables* décrivant la cible de déploiement.

[TIP]
.[RemarquePreTitre]#À savoir# `NODE_ENV`
====
La variable d'environnement `NODE_ENV` a été adoptée par de nombreux outils et _frameworks_ pour déterminer le contexte d'exécution d'une application, effectuer des optimisations ou afficher des informations de débug supplémentaires.

Il est *recommandé d'utiliser `NODE_ENV=production`… en production*.

* `development` +
  C'est le contexte assumé par défaut lorsqu'une personne développe sur sa machine.
  Les exceptions sont affichées de manière verbeuse et du code supplémentaire .
  Des informations sensibles peuvent être contenues dans les traces d'erreurs ;
* `test` +
  C'est la valeur que l'on choisit pour exécuter des tests applicatifs, augmenter la verbosité des _logs_ et activer les _Source Maps_ pour faciliter le débogage de fichiers transpilés.
  Certains _frameworks_ de test et outils d'_intégration continue_ assignent cette variable d'environnement par défaut ;
* `production` +
  Les exceptions n'affichent pas de détails – il faut aller <<exceptions,inspecter les erreurs et exceptions>>, l'outillage d'introspection est désactivé et la verbosité des _logs_ est supposée être moins verbeuse.
  On constate généralement des *améliorations de performance*.
====

L'exemple suivant décrit comment définir le _port réseau_ sur lequel écoutera le serveur HTTP.
Le numéro du port sera déterminé soit par le contexte d'exécution (_environnement de test_ ou _environnement de production_) soit par un numéro de port explicitement passée en tant que _variable d'environnement_.

[source%interactive,javascript,nodeVersion={nodeCurrentVersion}]
.config/env.js
----
include::{sourceDir}/config/env.js[]
----
<1> Décompose `PORT` de `process.env` et si la clé n'existe pas, assigne la valeur de `defaultPort`

Jouons avec le script `config/env.js` pour illustrer ces différents scénarios :

[source,bash]
----
$ PORT=8000 node config/env.js     # <1>
$ NODE_ENV=test node config/env.js # <2>
$ node config/env.js               # <3>
----
<1> Affiche `En écoute sur http://localhost:8000`
<2> Affiche `En écoute sur http://localhost:3001`
<3> Affiche `En écoute sur http://localhost:3000`

Le mécanisme des _variables d'environnement_ est détaillé dans la section <<../chapter-04/index.adoc#process-env,variables d'environnement>> du chapitre 4.

L'utilisation de fichiers de configuration peut se révéler être un bon _complément_ aux variables d'environnement.
Ces fichiers de configuration doivent *s'appliquer à tous les contextes d'exécution* (production comme développement) pour être au plus proche de l'environnement de production à tout moment mais aussi pour simplifier la maintenance.

Autrement dit *oui* à un fichier configurant des outils de la même manière pour tout le monde (eslint, babel etc.) mais *non* à un fichier _production.json_, _dev.json_ etc.

Le fichier `package.json` est adapté au stockage d'informations de configuration.
Il est idéal pour y stocker différentes _valeurs par défaut_ comme le montre l'exemple suivant avec la clé `config.port` :

[source,javascript]
.package.json#L6-12
----
include::package.json[lines=6..12, indent=0]
----

L'appel du fichier suivant affichera un résultat différent selon que l'invocation se fasse avec Node ou _npm_ (cf. <<../chapter-04/index.adoc#invoke,invoquer Node.js>>, Chapitre 4) :

[source,bash]
.Exécution de `config/file-npm.js` avec Node puis _npm_.
----
$ node config/file-npm.js
$ npm run config:file-npm
----

[source,javascript]
.config/file-npm.js
----
include::{sourceDir}/config/file-npm.js[]
----
<1> Affiche `undefined` avec Node et `3000` avec _npm_
<2> Affiche `3000` avec Node et _npm_

_npm_ aplatit la structure d'objet du fichier `package.json`, séparera chaque niveau de profondeur par le caractère `\_`, le préfixera par `npm_package_` et l'injectera dans l'objet `process.env`.

[TIP]
.[RemarquePreTitre]#Astuce# Combinaison avec les variables d'environnement
====
Un bon moyen de ne pas inscrire en dur l'emplacement d'un fichier de configuration est encore d'indiquer son emplacement via une variable d'environnement :

----
CONFIG_FILE=~/.secured/config.json node app.js
----
====

[[data-persistence]]
=== Persister les fichiers en dehors de notre application

Développer une application sur sa machine et la tester nous met à l'abri de certaines questions et considérations. +
Que se passe-t-il en cas de redémarrage de notre application ?
Et du système d'exploitation ?
Et si l'on doit installer l'application sur une nouvelle machine ?

On voudra s'assurer que nous ne perdrons pas de données lors d'une mise en production ou d'une mise à jour en production.
Autrement dit, il nous faut nous assurer que le bon support de persistence a été choisi pour les différents cas d'usage des données gérées.
Par exemple :

* une *liste d'utilisateurs inscrits* sur un service – on ne peut pas la perdre ;
* une *liste de sessions* – c'est acceptable de les remettre à zéro ;
* des *paramètres en cache* – on peut les reconstruire à la volée ;
* des *tâches de fond* – on veut les dissocier du processus applicatif.

*Arrêter et démarrer une application ne devrait pas entrainer de perte de données*.
Démarrer une application et l'accès à la source d'informations devrait *toujours fonctionner de la même manière* et ce, que ce soit son premier lancement, suite à une mise à jour ou après avoir déplacé l'application sur un différent support physique.

Enfin, il faut se poser la question de la _multiplicité_ : lancer plusieurs fois la même application Node—sur la même machine ou sur une machine différente—impacte-t-il la manière dont l'information est stockée et accédée ?
Cela entraine-t-il doublon ou corruption de données dans les valeurs enregistrées ?

Toutes ces questions devraient nous aiguiller vers le *choix d'un ou de plusieurs supports de persistence de données*.
Nous répondons au <<../chapter-07/index.adoc#database,choix d'une base de données>> dans le chapitre 7.
Nous verrons plus loin dans ce chapitre comment <<database-migration,effectuer des migrations de base de données>> afin de répercuter des évolutions dans leur structure et ce, sans intervention manuelle.



=== Exécuter l'application avec un utilisateur sans privilèges

Retenez une chose : *ne démarrez _jamais_ une application Node sur le port 80 ou 443* — ports utilisés par convention pour parler respectivement les protocoles HTTP et HTTPS.

L'exemple suivant illustre un serveur web Node qui écoutera sur le port 80 :

[source,javascript]
.server-port80.js
----
include::{sourceDir}/server-port80.js[]
----

Cela ne fonctionnera pas, y compris si le port est libre.
Tous les ports inférieurs à 1024 nécessitent que le programme soit lancé avec un utilisateur privilégié, comme _root_ par exemple.

Alors pourquoi ne pas utiliser _root_ pour avoir accès au port 80 ou 443 ?
<<system-security,Pour des raisons de sécurité>> comme évoqué précédemment dans ce chapitre.
Donc même si c'est techniquement faisable, *non nous le ferons pas* et nous privilégierons d'autres mécanismes, notamment celui du <<reverse-proxy,_reverse proxy_>>.


[TIP]
.[RemarquePreTitre]#Sécurité# Utilisateur sans privilèges
====
Un utilisateur sans privilège nommé `nobody` existe sur la plupart des systèmes d'exploitation.
Nous pouvons l'utiliser pour exécuter un processus critique. +
Cet utilisateur aura tout de même accès en lecture et en écriture aux fichiers et répertoires ouverts à tous les utilisateurs du système—l'impact devrait être limité.

----
$ sudo -u nobody node debug.js
$ ps aux | grep nobody
----

L'exécution de ces deux commandes lance un des exemples de ce chapitre en tant que `nobody`.
La liste des processus montre ensuite que le script `debug.js` est bien pris en charge par l'utilisateur `nobody`.
====

[TIP]
.[RemarquePreTitre]#Alternative# authbind
====
Le logiciel libre pour Linux _authbind_ comble le besoin impérieux d'assigner un port privilégié mais à un _utilisateur sans privilège_.

Il conviendra quand même de créer un utilisateur système avec des permissions limitées spécifiquement pour chaque application Node.

- [URL]#http://manpages.ubuntu.com/authbind#
====

[[database-migration]]
=== Scripter les changements de schémas de base de données

TBD.

=== Utiliser Docker pour tester un environnement reproductible

TBD.

[[startup]]
== Démarrer automatiquement nos applications

Qu'est-ce qui différencie le fonctionnement d'une application Node sur un _ordinateur de développement_ d'un _serveur de production_ ?
La manière de démarrer cette application.

Le démarrage d'une application Node sur notre machine de développement est une chose intuitive car manuelle.
Nous *démarrons le processus à la main*.
Nous avons d'ailleurs abordé différentes manières d'<<../chapter-04/index.adoc#invoke,invoquer un processus Node>> dans le <<../chapter-04/index.adoc#,chapitre 4>>.

Lorsqu'il s'agit d'un environnement de production, nous avons à _automatiser_ cette opération manuelle.

Nous allons voir différentes méthodes aboutissant à un résultat similaire.

=== Déléguer au service d'hébergement

Certains services d'hébergement prennent en charge le démarrage automatique de nos applications.
C'est le cas notamment des hébergements de type <<paas,_Platform as a Service_>> (_PaaS_) que nous explorerons plus en détails dans une section ci-après.

Ces services vont se baser sur des *conventions* ou des fichiers de *configuration* pour comprendre la nature de notre application et l'intégrer automatiquement.
Ils se baseront souvent sur le <<../chapter-05/index.adoc#npm-scripts,script `npm start`>> et d'autres informations contenues dans le fichier `package.json`.


[[process-manager]]
=== Avec un gestionnaire de processus

Les _gestionnaires de processus_ s'intègrent entre le système d'exploitation et nos applications.
Ils servent à *gérer leur démarrage* que ce soit en développement ou en production.
Ils déclarent nos applications en tant que *services système de manière universelle*, peu importe le système d'exploitation.

Autrement dit, un gestionnaire de processus prend en charge le *démarrage d'un processus* en tâche de fond, le *suivi de leur statut* ainsi que l'<<system-service,intégration en tant que *service système*>>.
Certains gestionnaires vont jusqu'à l'*inspection des logs* et le *passage à l'échelle* afin notamment de répartir les processus applicatifs sur chaque CPU de la machine.

*pm2* ([URL]#http://pm2.io/#) est un gestionnaire de processus populaire dans la communauté Node.
Il est lui-même écrit en ECMAScript et fonctionne aussi bien sous Linux, Windows que macOS.

La commande suivante démarrera une application web en lui attribuant un nom (ici, _nodebook-ch06-testapp_).
L'application sera démarrée en mode _cluster_ à raison d'un processus par CPU afin de répartir le trafic en fonction de la charge.

[source,bash]
----
$ pm2 start app.js -i 0 --name nodebook-ch06-testapp
----

.Liste des processus gérés par _pm2_.
image::images/pm2-status.png[width="85%"]

L'ordinateur servant à la rédaction de l'ouvrage dispose de 4 CPU donc _pm2_ crée autant de processus en parallèle.

L'application de démonstration affiche un nom aléatoire de Pokémon ainsi que l'identifiant du processus au sein du _cluster_ lors de la consultation de la page [URL]#http://localhost:4000#. +
On s'en rend compte notamment en utilisant le programme `curl` (ou équivalent) :

----
$ curl --location http://localhost:4000
> [cluster #2] Jynx

$ curl --location http://localhost:4000
> [cluster #1] Starmie
----

Les processus peuvent être stoppés indépendamment.
L'application peut être redémarrée automatiquement en cas de plantage, et sans interruption après une mise à jour de la base de code, entre autres.

[TIP]
.[RemarquePreTitre]#Documentation# pm2
====
La documentation du projet _pm2_ liste en détails les actions et modules du programme.
Des tutoriaux sont également à disposition pour décrire pas à pas différentes intégration, notamment avec le serveur HTTP _nginx_ ou d'autres langages comme _TypeScript_.

- `$ pm2 --help`
- [URL]#http://pm2.keymetrics.io/#
- [URL]#https://github.com/Unitech/pm2#
====

[TIP]
.[RemarquePreTitre]#Alternative# foreman
====
_foreman_ est un gestionnaire de processus très simple à appréhender.
Il est utilisé pour déclarer et démarrer des applications sur la plate-forme d'<<paas,hébergement Heroku>>.

- [URL]#https://rubygems.org/gems/foreman#
- [URL]#http://ddollar.github.io/foreman/#
====


[[system-service]]
=== En créant un service système

On l'oublierait presque mais il n'y a pas besoin d'installer de logiciel pour démarrer une application automatiquement.
Après tout, c'est une des *responsabilités du système d'exploitation*.

Chaque système d'exploitation se base sur un *gestionnaire de processus*.
Nous pouvons lui communiquer les informations relatives au lancement de notre programme :

* *quelle(s) commande(s) exécuter* ;
* dans *quel ordre*, à *quel moment* et *après* quel(s) service(s) ;
* la *stratégie de reprise* (laisser le processus hors-service, redémarrer automatiquement etc.) ;
* *quelles opérations* effectuer avant et/ou après son démarrage, arrêt, redémarrage.

Nous déclarons ces informations dans un fichier de configuration, sous forme textuelle, souvent à raison d'un fichier de configuration par programme.
La syntaxe varie selon les gestionnaires de processus mais la philosophie reste identique. +
*systemd* est l'outil de gestion de processus de la distribution link:http://www.ubuntu-fr.org/[Linux Ubuntu] mais aussi d'autres distributions populaires comme _Debian_, _Fedora_ et _CentOS_.

[source]
.systemd/nodebook.d/app.conf
----
include::{sourceDir}/systemd/nodebook.d/app.conf[]
----

L'exemple précédent décrit l'initialisation notre service après le démarrage du service en charge des interfaces réseau (`NetworkManager.service`).
_systemd_ exécute la commande `npm start` en tant qu'utilisateur `nobody` (sans privilèges donc) dans le répertoire précisé par la directive `WorkingDirectory` et injecte la variable d'environnement `NODE_ENV`.
Enfin, _systemd_ redémarrera le service en cas d'erreur, dans une limite de 5 tentatives en l'espace de 120 secondes.

Si le fichier de configuration est accessible en tant que `/etc/systemd/nodebook.d/app.conf`, le service `nodebook` peut être démarré manuellement comme suit :

[source,bash]
----
$ sudo systemctl start nodebook.service
----

D'autres commandes comme `reload`, `stop` et `restart` nous donnent le contrôle sur le cycle de vie de nos services. +
Dans tous les cas, le service sera démarré automatiquement au prochain démarrage du système d'exploitation.

[TIP]
.[RemarquePreTitre]#Documentation# systemd
====
Une introduction ainsi qu'une documentation détaillée de _systemd_ sont disponibles en ligne :

- [URL]#https://doc.ubuntu-fr.org/systemd# (en français)
- [URL]#https://freedesktop.org/software/systemd/man/# (en anglais)
====

[TIP]
.[RemarquePreTitre]#Alternative# Et pour Windows ?
====
La création d'un service système sur un serveur Windows implique une toute autre procédure.
Le service peut être manuellement créé depuis le *menu Services* ou scripté via l'intermédiaire du module npm `node-windows`.

- [URL]#https://npmjs.com/node-windows#
====


[[application-manager]]
=== Avec un serveur d'applications web

Un serveur d'application web a deux objectifs majeurs : *gérer le trafic HTTP* de manière optimale et *gérer le cycle de vie* d'une application web lors d'incidents ou de mises à jour par exemple.
Ce type de logiciel se _place entre_ une application Web et le monde extérieur.
Il peut être installé soit de manière *autonome* soit en tant que *module d'un serveur HTTP* tiers.

Leur *excellente gestion d'HTTP* permet de nous reposer sur les serveurs d'applications web pour gérer la charge des requêtes, de se protéger contre servir et mettre en cache les fichiers statiques, rester disponible même si l'application Node est tombée, entre autres.

Leur *intégration avec des plates-formes comme Node* alloue aux serveurs d'applications web la capacité de redémarrer une application Node si elle a planté ou a été mise à jour et ce, sans discontinuité de service (_zero downtime_).
Certains serveurs ont également la faculté de passer à l'échelle en *attribuant les applications Node aux CPU disponibles* afin de répartir la charge en fonction des ressources disponibles.

*Phusion Passenger* ([URL]#https://phusionpassenger.com/#) est un serveur d'applications web open source et compatible avec des applications Ruby, Node et Python.
Il s'installe de manière autonome ou en module pour _nginx_ ([URL]#http://nginx.org/#) et _Apache httpd_ ([URL]#https://httpd.apache.org/#).
Regardons ensemble l'intégration de _Passenger_ au travers de la configuration de son module _nginx_ :

[source]
.phusion-passenger/sites-available/webapp-nodebook.conf
----
include::{sourceDir}/phusion-passenger/sites-available/webapp-nodebook.conf[lines=2..-1]
----

L'exemple de configuration précédent déclare une application Node `app.js` placée dans le répertoire `/var/www/webapp-nodebook` et dont les fichiers statiques (JavaScript, CSS, images etc.) sont placés dans le sous-répertoire `public/`.

_Passenger_ démarrera au minimum 1 processus du fichier `app.js` dans une limite de 10, en fonction du traffic HTTP.
Chacun des processus sera démarré sous l'_utilisateur_ et le _groupe Unix_ propriétaire du fichier `app.js`.

_Passenger_ peut être contrôlé par l'intermédiaire du programme `passenger-config`, notamment avec la sous-commande `restart-app` :

[source,bash]
----
$ passenger-config restart-app /var/www/webapp-nodebook
----

[TIP]
.[RemarquePreTitre]#Documentation# Phusion Passenger
====
Un guide pas à pas de configuration d'une application Node avec Passenger est disponible sur leur site officiel.

- [URL]#https://phusionpassenger.com/library/walkthroughs/deploy/nodejs/#
- [URL]#https://phusionpassenger.com/library/config/nginx/reference/#
====

Je recommande fortement cette approche, relativement facile à installer et à configurer, notamment de manière automatisée avec un serveur d'intégration continue.

Nous trouverons des informations sur le fonctionnement de Node derrière un <<reverse-proxy,_reverse proxy_>> dans une autre section de ce chapitre.


[[monitoring]]
== Points de vigilance après une mise en ligne

Cette section s'intéresse à la *détection proactive des incidents* et au *suivi de la santé de notre application* une fois mise en ligne.
Nous allons ainsi apprendre à *dresser un diagnostic sans nous connecter* sur la ou les machines hébergeant l'application… en partie car nous n'y avons pas toujours accès.
Et c'est tant mieux.


[[uptime]]
=== Être prévenu·e quand l'application plante

TBD.

////
https://www.pingdom.com/free
https://uptimerobot.com/
////


.Message d'erreur du serveur _nginx_ lorsque l'application Node ne répond plus.
image::images/nginx-proxy-error.png[width="85%"]

  On cherchera entres autres à tester la réaction de notre code face à des erreurs provoquées volontairement.
* *ressources indisponibles* +
  Il arrive aussi que notre code ne soit pas incriminé… mais que la base de données devienne inaccessible pendant l'exécution de notre application.
  Ou qu'une clé d'API soit révoquée.
  Ou encore qu'une API soit indisponible, peu importe la raison—panne réseau, attaque destinée à faire tomber le système etc. +
  Le mieux que l'on puisse faire est d'utiliser des *modules _npm_ favorisant la reconnexion* aux bases de données, à *gérer l'indisponibilité de la ressource* (en ayant un modèle de réponse dégradé) ou encore en utilisant la technique du _backoff_ exponentiel pour retenter un certain nombre de fois en augmentant le temps entre temps chaque essai.
* *effets de bord système* +
  Le système d'exploitation peut nous jouer des tours sans que notre application ne soit réellement au courant.
  C'est le cas lorsque la _charge système_ dépasse la capacité de traitement des processeurs : notre application mettra plus de temps à exécuter toutes les opérations.
  Cela se produira également si l'espace disque tend à devenir faible : le système _swappera_ et ralentira l'écriture de fichiers, notamment les _logs_ générés par le trafic de notre application. +
  La mise en place en place de <<monitoring,_monitoring_>> nous aidera à être *prévenu _avant_* que le problème ne se déclare… ou au moins d'avoir une vue d'ensemble des ressources disponibles de notre parc machine.


[[exceptions]]
=== Consulter les erreurs applicatives

TBD.

////
https://sentry.io + https://docs.sentry.io/clients/node/
https://newrelic.com/nodejs
////

[[security.node]]
=== Mettre à jour Node en cas de faille de sécurité

////
https://nodejs.org/en/security/
////


[[security.npm]]
=== Mettre à jour les modules npm en cas de faille de sécurité

Il existe quatre niveaux où des failles de sécurité peuvent s'immiscer :

* *notre propre code* +
  Des revues de code, une amélioration de vos connaissances et la commande d'audits nous aideront à identifier les possibles failles et vulnérabilités ;
* *nos dépendances* +
  Mais aussi dans les dépendances de nos dépendances ;
* *Node* +
  Le code de certains modules Node – ou leur intégration avec un système d'exploitation spécifique – contient des failles ou fuites mémoire qui sont corrigées au fil des versions ;
* *dépendances de Node*
  Il est arrivé à plusieurs reprises que des vulnérabilités soient décelées dans _libuv_, _OpenSSL_ ou _V8_ — seule une mise à jour de Node peut mettre à ces dépendances.

On n'oubliera pas les vulnérabilités liés à notre système d'exploitation et celui de nos serveurs de production ainsi qu'à leur exposition au monde extérieur.

Plusieurs canaux sont à notre disposition pour être notifié très rapidement de mises à jour correctives :

* *Failles de sécurité de modules npm* : [URL]#https://nodesecurity.io/advisories# — et son flux RSS [URL]#https://nodesecurity.io/rss.xml# ;
* *Failles de sécurité de Node* : son flux RSS [URL]#https://nodejs.org/en/feed/vulnerability.xml# ;
* *Mises à jour de Node* : [URL]#https://nodejs.org/en/blog/# — et son flux RSS [URL]#https://nodejs.org/en/feed/blog.xml#.

////
- https://snyk.io/
- https://nodesecurity.io/
- greenkeeper
////


== Conclusion

TBD.
